import sqlite3
import json
import re
import asyncio
import logging
import os
from datetime import datetime
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    ContextTypes,
    filters,
    CallbackQueryHandler
)
from cachetools import TTLCache
import redis
from typing import Dict, List, Optional

# ---------------------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ----------------------
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

DATABASE_NAME = 'v2.db'
JSON_DATA_SOURCE = 'input.json'
MAX_MESSAGE_LENGTH = 4096  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…

CACHE_CONFIG = {
    'maxsize': 1000,
    'ttl': 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
}

SEARCH_CONFIG = {
    'result_limit': 5,
    'max_display': 20,
    'min_query_length': 1,
    'rate_limit': 15  # Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§ Ù„ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
}

REDIS_CONFIG = {
    'host': 'localhost',
    'port': 6379,
    'db': 0,
    'decode_responses': True,
    'socket_timeout': 5,
    'socket_connect_timeout': 5
}

# ---------------------- ÙØ¦Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ----------------------
class HadithDatabase:
    """ÙØ¦Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ Ø¥Ø¯Ø§Ø±Ø© Ø¹Ù…Ù„ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§ØªØµØ§Ù„Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ùˆ Redis"""
        try:
            self.redis = redis.Redis(**REDIS_CONFIG)
            self.redis.ping()  # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
            
            self.conn = sqlite3.connect(
                DATABASE_NAME,
                check_same_thread=False,
                isolation_level=None,
                timeout=30
            )
            self.conn.row_factory = sqlite3.Row
            self._initialize_database()
            
        except Exception as e:
            logger.error(f"ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
            raise

    def _initialize_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„ÙÙ‡Ø§Ø±Ø³"""
        with self.conn:
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠØ« Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            self.conn.execute('''
                CREATE TABLE IF NOT EXISTS hadiths (
                    id INTEGER PRIMARY KEY,
                    book TEXT NOT NULL,
                    text TEXT NOT NULL,
                    grading TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )''')
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ÙÙˆØ±ÙŠ
            self.conn.execute('''
                CREATE VIRTUAL TABLE IF NOT EXISTS hadiths_fts 
                USING fts5(text, content='hadiths')''')
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.conn.execute('''
                CREATE TABLE IF NOT EXISTS stats (
                    type TEXT PRIMARY KEY,
                    count INTEGER DEFAULT 0,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )''')
            
            # ÙÙ‡Ø§Ø±Ø³ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
            self.conn.execute('CREATE INDEX IF NOT EXISTS idx_book ON hadiths(book)')
            self.conn.execute('CREATE INDEX IF NOT EXISTS idx_created ON hadiths(created_at)')
            self.conn.execute('CREATE INDEX IF NOT EXISTS idx_book_grading ON hadiths(book, grading)')
            
        self._load_initial_data()
    
    def _load_initial_data(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ù…Ù„Ù JSON"""
        try:
            with open(JSON_DATA_SOURCE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            current_data = self.conn.execute('SELECT COUNT(*) FROM hadiths').fetchone()[0]
            if current_data == 0:
                logger.info("Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©...")
                self._import_data(data)
                logger.info("ØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
                
        except Exception as e:
            logger.error(f'Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}')
            raise
    
    def _import_data(self, data: List[Dict]):
        """Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        with self.conn:
            # ØªÙØ±ÙŠØº Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
            self.conn.execute('DELETE FROM hadiths')
            self.conn.execute('DELETE FROM hadiths_fts')
            self.conn.execute('DELETE FROM stats')
            
            batch = []
            for item in data:
                clean_text = self._sanitize_text(item.get('arabicText', ''))
                batch.append((
                    item.get('book', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'),
                    clean_text,
                    item.get('majlisiGrading', 'ØºÙŠØ± Ù…ØµÙ†Ù')
                ))
                
                if len(batch) >= 500:
                    self._insert_batch(batch)
                    batch = []
            
            if batch:
                self._insert_batch(batch)
            
            # ØªØ­Ø¯ÙŠØ« ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«
            self.conn.execute('''
                INSERT INTO hadiths_fts (rowid, text)
                SELECT id, text FROM hadiths
            ''')
            self.conn.execute('INSERT INTO hadiths_fts(hadiths_fts) VALUES(\'rebuild\')')
            
    def _insert_batch(self, batch: List[tuple]):
        """Ø¥Ø¯Ø®Ø§Ù„ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        self.conn.executemany('''
            INSERT INTO hadiths (book, text, grading)
            VALUES (?, ?, ?)
        ''', batch)
    
    def _sanitize_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        text = re.sub(r'[\u064B-\u065F\u0610-\u061A]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'\s+', ' ', text).strip()  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        return self.normalize_arabic(text)
    
    @staticmethod
    def normalize_arabic(text: str) -> str:
        """ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø¨Ø­Ø«"""
        replacements = {'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§', 'Ø©': 'Ù‡'}
        for old, new in replacements.items():
            text = text.replace(old, new)
        return text
    
    def search_hadiths(self, query: str) -> List[Dict]:
        """
        Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„Ù„ÙˆØ§Ùˆ ÙˆØ§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        
        Args:
            query (str): Ù†Øµ Ø§Ù„Ø¨Ø­Ø«
            
        Returns:
            list: Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« ÙƒÙ‚Ø§Ø¦Ù…Ø© Ù…Ù† dictionaries
        """
        normalized_query = self.normalize_arabic(query)
        cache_key = f'search:{normalized_query}'
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø£ÙˆÙ„Ø§Ù‹
        if cached := self.redis.get(cache_key):
            return json.loads(cached)
            
        terms = []
        for term in normalized_query.split():
            term = term.strip()
            if not term:
                continue
                
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ§Ùˆ Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ©
            if term.startswith('Ùˆ'):
                variants = [term, term[1:]] if len(term) > 1 else [term]
            else:
                variants = [term, f'Ùˆ{term}']
                
            terms.append(f'({" OR ".join(variants)})')
        
        if not terms:
            return []
        
        fts_query = ' AND '.join(terms)
        
        try:
            with self.conn:
                results = self.conn.execute('''
                    SELECT book, text, grading 
                    FROM hadiths
                    WHERE id IN (
                        SELECT rowid 
                        FROM hadiths_fts 
                        WHERE hadiths_fts MATCH ?
                        ORDER BY bm25(hadiths_fts)
                        LIMIT 1000
                    )
                ''', (fts_query,)).fetchall()
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù‚ÙˆØ§Ù…ÙŠØ³ ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ Redis
                results_dict = [dict(row) for row in results]
                self.redis.setex(cache_key, 300, json.dumps(results_dict))
                return results_dict
                
        except Exception as e:
            logger.error(f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {str(e)}')
            return []
    
    def update_statistics(self, stat_type: str):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Redis"""
        try:
            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯ ÙÙŠ Redis
            self.redis.zincrby('stats', 1, stat_type)
            
            # Ù…Ø²Ø§Ù…Ù†Ø© Ù…Ø¹ SQLite ÙƒÙ„ 10 Ø·Ù„Ø¨Ø§Øª
            if self.redis.incr(f'stats_sync:{stat_type}') % 10 == 1:
                self.redis.expire(f'stats_sync:{stat_type}', 3600)
                count = int(self.redis.zscore('stats', stat_type) or 0)
                
                # ØªØ®Ø²ÙŠÙ† ÙÙŠ SQLite
                with self.conn:
                    self.conn.execute('''
                        INSERT INTO stats (type, count) 
                        VALUES (?, ?)
                        ON CONFLICT(type) DO UPDATE SET 
                            count = excluded.count,
                            last_updated = CURRENT_TIMESTAMP
                    ''', (stat_type, count))
                    
        except Exception as e:
            logger.error(f'Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {str(e)}')
    
    def get_statistics(self) -> Dict[str, int]:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ù† SQLite"""
        try:
            with self.conn:
                return {row['type']: row['count'] 
                        for row in self.conn.execute('SELECT type, count FROM stats')}
        except Exception as e:
            logger.error(f'Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {str(e)}')
            return {}

    def bulk_update(self, operations: List[Dict]):
        """ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø¹Ù„Ù‰ Redis Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©"""
        try:
            pipe = self.redis.pipeline()
            for op in operations:
                if op['type'] == 'increment':
                    pipe.incr(op['key'])
                elif op['type'] == 'set':
                    pipe.set(op['key'], op['value'], ex=op.get('ttl'))
            pipe.execute()
        except Exception as e:
            logger.error(f'Ø®Ø·Ø£ ÙÙŠ bulk_update: {str(e)}')

# ---------------------- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¨ÙˆØª ----------------------
db = HadithDatabase()

async def check_rate_limit(user_id: int) -> bool:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    key = f"ratelimit:{user_id}"
    current = db.redis.incr(key)
    if current == 1:
        db.redis.expire(key, 60)
    return current > SEARCH_CONFIG['rate_limit']

def split_text(text: str, max_length: int = MAX_MESSAGE_LENGTH) -> List[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø·ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡"""
    parts = []
    while len(text) > max_length:
        split_index = text.rfind(' ', 0, max_length)
        split_index = split_index if split_index != -1 else max_length
        parts.append(text[:split_index])
        text = text[split_index:].lstrip()
    parts.append(text)
    return parts

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ù…Ø± /start"""
    user = update.effective_user
    keyboard = [[InlineKeyboardButton(
        "â• Ø£Ø¶ÙÙ†ÙŠ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ØªÙƒ", 
        url=f"t.me/{context.bot.username}?startgroup=true"
    )]]
    
    welcome_message = f"""
    <b>Ù…Ø±Ø­Ø¨Ø§ {user.first_name}!
    Ø£Ù†Ø§ Ø¨ÙˆØª ÙƒØ§Ø´Ù Ø£Ø­Ø§Ø¯ÙŠØ« Ø§Ù„Ø´ÙŠØ¹Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙŠ Ø§ÙƒØ«Ø± Ù…Ù† 26155 Ø­Ø¯ÙŠØ« ğŸ”</b>

    <i>Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¨ÙˆØª:</i>
    - ÙƒØªØ§Ø¨ Ø§Ù„ÙƒØ§ÙÙŠ Ù„Ù„ÙƒÙ„ÙŠÙ†ÙŠ Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ Ù…Ù† Ù…Ø±Ø§Ø© Ø§Ù„Ø¹Ù‚ÙˆÙ„ Ù„Ù„Ù…Ø¬Ù„Ø³ÙŠ
    - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø­Ø§Ø¯ÙŠØ« Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø¹ÙŠÙˆÙ† Ø§Ø®Ø¨Ø§Ø± Ø§Ù„Ø±Ø¶Ø§ Ù„Ù„ØµØ¯ÙˆÙ‚
    - ÙƒØªØ§Ø¨ Ù†Ù‡Ø¬ Ø§Ù„Ø¨Ù„Ø§ØºØ©
    - ÙƒØªØ§Ø¨ Ø§Ù„Ø®ØµØ§Ù„ Ù„Ù„ØµØ¯ÙˆÙ‚ 
    - ÙˆØ³ÙŠØªÙ… Ø§Ø¶Ø§ÙØ© Ø¨Ø§Ù‚ÙŠ ÙƒØªØ¨ Ø§Ù„Ø´ÙŠØ¹Ø©
    - ÙƒØªØ§Ø¨ Ø§Ù„Ø§Ù…Ø§Ù„ÙŠ Ù„Ù„ØµØ¯ÙˆÙ‚
    - ÙƒØªØ§Ø¨ Ø§Ù„Ø§Ù…Ø§Ù„ÙŠ Ù„Ù„Ù…ÙÙŠØ¯
    - ÙƒØªØ§Ø¨ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ù„Ù„ØµØ¯ÙˆÙ‚
    - ÙƒØªØ§Ø¨ ÙØ¶Ø§Ø¦Ù„ Ø§Ù„Ø´ÙŠØ¹Ø© Ù„Ù„ØµØ¯ÙˆÙ‚
    - ÙƒØªØ§Ø¨ ÙƒØ§Ù…Ù„ Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ù„Ø§Ø¨Ù† Ù‚ÙˆÙ„ÙˆÙŠÙ‡ Ø§Ù„Ù‚Ù…ÙŠ
    - ÙƒØªØ§Ø¨ Ø§Ù„Ø¶Ø¹ÙØ§Ø¡ Ù„Ø§Ø¨Ù† Ø§Ù„ØºØ¶Ø§Ø¦Ø±ÙŠ
    - ÙƒØªØ§Ø¨ Ø§Ù„ØºÙŠØ¨Ø© Ù„Ù„Ù†Ø¹Ù…Ø§Ù†ÙŠ
    - ÙƒØªØ§Ø¨ Ø§Ù„ØºÙŠØ¨Ø© Ù„Ù„Ø·ÙˆØ³ÙŠ
    - ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ø¤Ù…Ù† Ù„Ø­Ø³ÙŠÙ† Ø¨Ù† Ø³Ø¹ÙŠØ¯ Ø§Ù„ÙƒÙˆÙÙŠ Ø§Ù„Ø§Ù‡ÙˆØ§Ø²ÙŠ
    - ÙƒØªØ§Ø¨ Ø§Ù„Ø²Ù‡Ø¯ Ù„Ø­Ø³ÙŠÙ† Ø¨Ù† Ø³Ø¹ÙŠØ¯ Ø§Ù„ÙƒÙˆÙÙŠ Ø§Ù„Ø§Ù‡ÙˆØ§Ø²ÙŠ
    - ÙƒØªØ§Ø¨ Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø§Ø®Ø¨Ø§Ø± Ù„Ù„ØµØ¯ÙˆÙ‚
    - ÙƒØªØ§Ø¨ Ù…Ø¹Ø¬Ù… Ø§Ù„Ø£Ø­Ø§Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹ØªØ¨Ø±Ø© Ù„Ù…Ø­Ù…Ø¯ Ø£ØµÙØ± Ù…Ø­Ø³Ù†ÙŠ
    - ÙƒØªØ§Ø¨ Ù†Ù‡Ø¬ Ø§Ù„Ø¨Ù„Ø§ØºØ© Ù„Ø¹Ù„ÙŠ Ø¨Ù† Ø£Ø¨ÙŠ Ø·Ø§Ù„Ø¨
    - ÙƒØªØ§Ø¨ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù„Ù„Ø¥Ù…Ø§Ù… Ø²ÙŠÙ† Ø§Ù„Ø¹Ø§Ø¨Ø¯ÙŠÙ†

    <b>Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:</b>
    <code>Ø´ÙŠØ¹Ø© [Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ]</code>

    <b>Ù…Ø«Ø§Ù„:</b>
    <code>Ø´ÙŠØ¹Ø© Ø¨Ø§Ù‡ØªÙˆÙ‡Ù…</code>

    <i>Ù‚Ù†Ø§Ø© Ø§Ù„Ø¨ÙˆØª</i>
    @shia_b0t
    Ø§Ø¯Ø¹Ùˆ Ù„ÙˆØ§Ù„Ø¯ÙŠ Ø¨Ø§Ù„Ø±Ø­Ù…Ø© Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒÙ… Ø¥Ù† Ø§Ø³ØªÙØ¯ØªÙ… Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…Ù„
    """
    
    try:
        await update.message.reply_html(
            welcome_message,
            reply_markup=InlineKeyboardMarkup(keyboard),
            disable_web_page_preview=True
        )
        db.update_statistics('start')
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ start_command: {str(e)}")

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ù…Ø± /help"""
    try:
        stats = db.get_statistics()
        response = f"""
        <b>ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:</b>
        â€¢ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø«: <code>{stats.get('search', 0)}</code>
        â€¢ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: <code>{stats.get('start', 0)}</code>
        """
        await update.message.reply_html(response)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ help_command: {str(e)}")
        await update.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")

async def handle_search(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨Ø­Ø«"""
    try:
        user = update.effective_user
        if await check_rate_limit(user.id):
            await update.message.reply_text("â³ ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ù…Ù† Ø§Ù„Ø·Ù„Ø¨Ø§Øª! Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±...")
            return
            
        if not update.message.text.startswith('Ø´ÙŠØ¹Ø©'):
            return
            
        query = update.message.text[4:].strip()
        if not query or len(query) < SEARCH_CONFIG['min_query_length']:
            await update.message.reply_text("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„Ø¨Ø­Ø« (3 Ø£Ø­Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)")
            return
            
        db.update_statistics('search')
        
        results = db.search_hadiths(query)
        total = len(results)
        
        if not results:
            await update.message.reply_html("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")
            return
            
        if total > SEARCH_CONFIG['max_display']:
            await update.message.reply_html(
                f"<b>âš ï¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {total} Ù†ØªÙŠØ¬Ø©!</b>\n"
                "Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ¶ÙŠÙŠÙ‚ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø£Ø®Ø±Ù‰."
            )
            return
            
        response = [f"<b>ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {total} Ù†ØªÙŠØ¬Ø©:</b>\n"]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚
        for idx, hadith in enumerate(results[:10], 1):
            text = hadith['text']
            match_index = text.find(query)
            
            if match_index != -1:
                start = max(0, match_index - 20)
                end = min(len(text), match_index + 50)
                snippet = text[start:end].replace(query, f"<b>{query}</b>")
                response.append(
                    f"{idx}. {snippet}\n"
                    f"ğŸ“š Ø§Ù„ÙƒØªØ§Ø¨: {hadith['book']}\n"
                    f"ğŸ“Œ ØµØ­Ø© Ø§Ù„Ø­Ø¯ÙŠØ«: {hadith['grading']}\n"
                )
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¶Ù…Ù† Ø§Ù„Ø­Ø¯
        if total <= SEARCH_CONFIG['result_limit']:
            for hadith in results:
                message = (
                    f"<b>ğŸ“– {hadith['book']}</b>\n\n"
                    f"{hadith['text']}\n\n"
                    f"<i>ØµØ­Ø© Ø§Ù„Ø­Ø¯ÙŠØ«: {hadith['grading']}</i>"
                )
                for part in split_text(message):
                    await update.message.reply_html(part)
                    await asyncio.sleep(0.1)
        else:
            response.append("\n<b>â„¹ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„Ù…ØªÙ† Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£Ø¯Ù‚.</b>")
            await update.message.reply_html('\n'.join(response))
            
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ handle_search: {str(e)}")
        await update.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ")

async def real_time_analytics():
    """Ù…Ù‡Ù…Ø© Ø®Ù„ÙÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ"""
    while True:
        try:
            messages = db.redis.xread({'analytics_stream': '$'}, block=0, count=10)
            for stream, message_list in messages:
                for message_id, message_data in message_list:
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠØ©
                    logger.info(f"ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª: {message_data}")
                    db.redis.xdel('analytics_stream', message_id)
                    
            await asyncio.sleep(5)
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ real_time_analytics: {str(e)}")
            await asyncio.sleep(10)

# ---------------------- Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ----------------------
def initialize_bot():
    """ØªÙ‡ÙŠØ¦Ø© ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª"""
    try:
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙƒÙ† Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
        token = os.getenv('BOT_TOKEN', '7378891608:AAGEYCS7lCgukX8Uqg9vH1HLMWjiX-C4HXg')
        
        application = Application.builder().token(token).build()
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø£ÙˆØ§Ù…Ø±
        application.add_handler(CommandHandler('start', start_command))
        application.add_handler(CommandHandler('help', help_command))
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
        application.add_handler(MessageHandler(
            filters.TEXT & ~filters.COMMAND,
            handle_search
        ))
        
        # Ø¨Ø¯Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
        application.job_queue.run_once(
            lambda _: asyncio.create_task(real_time_analytics()),
            when=5
        )
        
        logger.info("Ø¬Ø§Ø±ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª...")
        application.run_polling(
            poll_interval=1.0,
            timeout=30,
            drop_pending_updates=True
        )
        
    except Exception as e:
        logger.critical(f"ÙØ´Ù„ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª: {str(e)}")

if __name__ == '__main__':
    try:
        initialize_bot()
    except KeyboardInterrupt:
        logger.info("Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨ÙˆØª...")
    except Exception as e:
        logger.critical(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}")